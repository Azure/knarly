name: pod-churn

# This pod churn test can be set to target a particular level of churn (e.g. 50 pod operations per second) with the TARGET_POD_CHURN 
# parameter.  But if you just want it to go has fast as it can, set that to something high (e.g. a few thousand)
#
# Note that, after each round of "churn" - i.e. each time it shuts down existing pods and tells new ones to start, the test waits until the 
# new ones are actually running. So you'll see burst of create/delete, then pauses while the created ones start. Therefore, to get something
# approximating continuous churn at a steady rate, you would need to either
# (a) reduce TARGET_POD_CHURN low enough that the pods just start up at the rate they are being created OR
# (b) tweak the controller throttling/settings in your cluster so that pods can be started up more quickly.

# input params (which by default come from )
{{$ACTIVE_PODS_PER_NODE := DefaultParam .CL2_PODS_PER_NODE 60}}
{{$TARGET_POD_CHURN := DefaultParam .CL2_TARGET_POD_CHURN 10}}  # i.e. target pod churn, in mutations/sec, for cluster as a whole. (create+update+delete operations per second)

# computed params
{{$desiredConcurrentPods := MultiplyInt .Nodes $ACTIVE_PODS_PER_NODE}}  #Total number of active pods for cluster
{{$targetPodCreationsPerSecond := DivideInt $TARGET_POD_CHURN 4 }}  # The divisor here has been chosen by experimentation.  It allows for the create, the delete, and some patch operations that happen, and build the total pod churn (create, update, delete) up to approx the target


namespace:
  number: 1  # testing everything in one namespace

tuningSets:
- name: TargetCreateQps
  qpsLoad:
    qps: {{$targetPodCreationsPerSecond}}
- name: TargetDeleteQps
  qpsLoad:
    qps: {{$targetPodCreationsPerSecond}} # has same numerical value as create, but is separate tuning set so neither can starve the other
- name: Fast
  qpsLoad:
    qps: 4000    

steps:

#### Log params ###
# Can't find a log action, but the above name should function like a log, to let us see the computeed sleep seconds
- name: Log - pod creations {{$targetPodCreationsPerSecond}}/s, concurrent pods {{$desiredConcurrentPods}}
  measurements:
  - Identifier: Dummy
    Method: Sleep
    Params:
      action: start
      duration: 1ms

### Prepare, by starting up the desired number of concurrent pods. Let it stablize with all these running, before we start churning them.
#   This also has the effect on getting the container image onto all the nodes, so in the later stages, when we are measuring stuff,
#   image pull times will not be an issue
- name: Prepare - deploy initial pod set
  phases:
  - namespaceRange:
      min: 1
      max: 1
    replicasPerNamespace: {{$desiredConcurrentPods}}
    tuningSet: Fast  # no point in slowing this down, since we haven't even started the real part of the test yet  (this is the only difference between this and the module we use below)
    objectBundle:
    - basename: pods-set-0-instance  # naming is important because its how we delete them later (instance number suffix will be automatically added)
      objectTemplatePath: "naked-pod.yaml"
      templateFillMap:
        testRound: r0
- name: Prepare - wait until initial set of pods are running  
  measurements:
  - Identifier: WaitForRunningPods  # unlike WaitForControlledPods, this does not seem to require separate setup and gather phases
    Method: WaitForRunningPods
    Params:
      desiredPodCount: {{$desiredConcurrentPods}}
      labelSelector: test-round = r0
      timeout: 10m

### Initialize measurements
- name: Initialize measurements
  measurements:
  # See comment in Gather section below
  #- Identifier: SchedulingMetrics
  #  Method: SchedulingMetrics
  #  Params:
  #    action: start  
  #- Identifier: MetricsForE2E  
  #  Method: MetricsForE2E
  #  Params:
  #    action: start
  #    gatherKubeletsMetrics: false 
  - Identifier: SchedulingThroughput
    Method: SchedulingThroughput
    Params:
      action: start


### Test, in multiple rounds of "churn creation". Each round concurrently shuts down one set of pods and creates another.
#   We use multiple rounds so that we have a decent time period to measure, even in cases where churn rate (and therefore pod lifetimes)
#   is low.
- module:
    path: /churn-module.yaml
    params:
      replicas: {{$desiredConcurrentPods}}
      roundNumber: 1
- module:
    path: /churn-module.yaml
    params:
      replicas: {{$desiredConcurrentPods}}
      roundNumber: 2
- module:
    path: /churn-module.yaml
    params:
      replicas: 0   # zero replicas in the new round. Force deletion under our control (for faster cleanup than the default auto cleanup)
      roundNumber: 3

### Gather measurements
- name: Gather measurements
  measurements:
 # not supported on AKS by the looks (ask Ace, he knows about it, I see from Git commit history) 
 # - Identifier: SchedulingMetrics
 #   Method: SchedulingMetrics
 #   Params:
 #     action: gather
 #
 # Doesn't work well on AKS by the looks. Maybe if I can "register" the API server with CL2 it will work better (complains of it being )
 # unregistered.
 # - Identifier: MetricsForE2E
 #   Method: MetricsForE2E
 #   Params:
 #     action: gather   
 #
  - Identifier: SchedulingThroughput
    Method: SchedulingThroughput
    Params:
      action: gather   

- name: Pause before auto namespace cleanup # we've already deleted everything in it, but somehow K8s still seems to take ages (e.g 10+ mins in some cases) to delete the namespace. Lets try a little pause, to let things quiet down before we end the test
  measurements:
  - Identifier: Dummy
    Method: Sleep
    Params:
      action: start
      duration: 1m 