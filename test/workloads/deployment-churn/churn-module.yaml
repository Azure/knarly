# Cluster loader 2 module, for one phase of "churn"
# In a phase of churn, we shut down all the pods with one name pattern, and concurrently start up another with a new name patter
#
# We have to do it in phases like this because deletion in CL2 is name-based, and so we need the things we are deleting right now
# to have a different 'base name' from the things we are creating.

# TODO: tidy usage of $.field vs .field (for accessing data passed in from config.yaml).  Since we have no loops here, and so
# no new scopes defined, both should be equivalent - so it would be cleanest to standardize on one, probably the one without the $

{{$previousRoundNum := SubtractInt $.roundNumber 1}}
{{$fullRoundName := print $.desc " (rnd " $.roundNumber ")"}}
# compute a slightly lower replicas per NS number, to represent the level we want to "pre-delete" to
{{$preDelTargetOldReplicas := MaxInt (MultiplyInt $.newReplicas 0.9) $.oldReplicasAfterDeletion}} 
# compute how many pods (not deployments) we want to see in total finally, and also after pre-deletion
{{$finalPodCount        := MultiplyInt (AddInt $.newReplicas $.oldReplicasAfterDeletion) (MultiplyInt $.podsPerDeployment $.nsCount)}}
{{$preDelTargetPodCount := MultiplyInt       $preDelTargetOldReplicas                    (MultiplyInt $.podsPerDeployment $.nsCount)}}

steps:
- name: begin CRUD timer 
  measurements:
  - Identifier: Timer # (reference to timer named "Timer" in containing file)
    Method: Timer
    Params:
      action: start
      label: CRUD for {{$fullRoundName}}

{{if ne $.roundNumber 0.0}}
# Before we do the real churn, pre-delete a few of the ones we don't want
# If we don't do this, then we end up with a total number of pods that is greater than
# what we really want.  Comprised of created new ones PLUS some delete ones that haven't quite gone away yet.
# This causes the Cluster Autoscaler to have to scale out more, during the churn phase of the test - and we 
# don't want that because it skews our result. So here, we pre-delete a few pods.
# NOTE: when you're first learning how this test works, you can safely skip this section, and go 
# straight to "Do the real churn test" below.
- name: pre-deletion
  phases:
  - namespaceRange:
      min: 1
      max: {{.nsCount}}
    replicasPerNamespace: {{$preDelTargetOldReplicas}}  # set target of having slightly fewer pods than normal, from the previous round
    tuningSet: TargetDeleteQps  # (reference to tuning set from containing file)
    objectBundle:
    - basename: deployment-rnd-{{$previousRoundNum}}-instance  # delete the previous set
      objectTemplatePath: "deployment.yaml"
- name: wait for pre-deletion  # wait to make sure the pre-deletion really has worked
  measurements:
  - Identifier: WaitForRunningPods 
    Method: WaitForRunningPods
    Params:
      desiredPodCount: {{$preDelTargetPodCount}} 
      labelSelector: test-round in (r{{$previousRoundNum}})  
      timeout: {{$.timeout}}  
{{end}}          

# Do the real churn test
- name: {{$fullRoundName}}
  phases:  # phases run concurrently if they are within the same step
  - namespaceRange:
      min: 1
      max: {{.nsCount}}
    replicasPerNamespace: {{$.oldReplicasAfterDeletion}}
    tuningSet: TargetDeleteQps  # (reference to tuning set from containing file)
    objectBundle:
    - basename: deployment-rnd-{{$previousRoundNum}}-instance  # delete the previous set
      objectTemplatePath: "deployment.yaml"  
  - namespaceRange:
      min: 1
      max: {{.nsCount}}
    replicasPerNamespace: {{$.newReplicas}} 
    tuningSet: TargetCreateQps # (reference to tuning set from containing file)
    objectBundle:
    - basename: deployment-rnd-{{$.roundNumber}}-instance  # create the new set
      objectTemplatePath: "deployment.yaml"
      templateFillMap:
        testRound: r{{$.roundNumber}}
        replicas: {{$.podsPerDeployment}}

- name: end CRUD timer
  measurements:
  - Identifier: Timer # (ref to object in containing file)        
    Method: Timer
    Params:
      action: stop
      label: CRUD for {{$fullRoundName}}
- name: begin wait timer
  measurements:
  - Identifier: Timer
    Method: Timer
    Params:
      action: start
      label: wait for pods {{$fullRoundName}}

- name: Wait for all pods from round {{$.roundNumber}} to start running # otherwise we'll just delete them in the next round before they have even started running, but we'd rather exercise the full pod lifecycle
  measurements:
  - Identifier: WaitForRunningPods  # unlike WaitForControlledPods, this does not seem to require separate setup and gather phases
    Method: WaitForRunningPods
    Params:
      desiredPodCount: {{$finalPodCount}} 
      labelSelector: test-round in (r0, r{{$.roundNumber}})  # since we may carry some over from round 0
      timeout: {{$.timeout}}

- name: end wait timer
  measurements:
  - Identifier: Timer
    Method: Timer
    Params:
      action: stop
      label: wait for pods {{$fullRoundName}}
