name: deployment-churn
# Churn test with deployments
# This test works like the pod-churn one, but it groups the pods together into deployments.
# Note that, for high values of PODS_PER_DEPLOYMENT its interaction with the API server will get 
# quite chunky. E.g. if it makes a deployment with 1000 pods, that's just one operation, and this test
# does not control or define the rate at which those pods get created. (I.e. this test only controls the rate
# at which _deployments_ are created.)

# input params (which be default come from override file or CL2... env vars)
{{$ACTIVE_PODS_PER_NODE := DefaultParam .CL2_PODS_PER_NODE 60}}
{{$PODS_PER_DEPLOYMENT := DefaultParam .CL2_PODS_PER_DEPLOYMENT 100}}
{{$TARGET_POD_CHURN := DefaultParam .CL2_TARGET_POD_CHURN 10}}  # i.e. target pod churn, in mutations/sec, for cluster as a whole. (create+update+delete operations per second)
{{$POD_START_TIMEOUT_MINS := DefaultParam .CL2_POD_START_TIMEOUT_MINS 5}}  # how long to wait, at end of a phase, for its pods to start up
{{$CHURN_FRACTION := DefaultParam .CL2_CHURN_FRACTION 0.3}}
{{$CLEANUP := DefaultParam .CL2_CLEANUP 1}}
{{$NS_COUNT := DefaultParam .CL2_NS_COUNT 10}}  
# Note on CLEANUP of pods and deployments that we create.
# By default we do clean them up explicitly, because by default letting them just get deleted with the namespace is slow.
# However, if delete-collection-workers has been set to a highish value (e.g. 250) in the API server params, then 
# there is no need for us to delete them explicitly. They'l be deleted quickly when this test deletes its namespace.
# So, if you have set that parameter in your API server, then set the CL2_CLEANUP parameter to 0

# computed params
{{$desiredConcurrentPods := MultiplyInt .Nodes $ACTIVE_PODS_PER_NODE}}  #Total number of active pods for cluster
{{$targetPodCreationsPerSecond := DivideInt $TARGET_POD_CHURN 6 }}  # The divisor here has been chosen by experimentation.  It allows for the create, the delete, and some patch operations that happen, and build the total pod churn (create, update, delete) up to approx the target
{{$targetDeploymentCreationsPerSecond := DivideFloat $targetPodCreationsPerSecond $PODS_PER_DEPLOYMENT}}
{{$desiredConcurrentDeployments := MaxInt 1 (DivideFloat $desiredConcurrentPods $PODS_PER_DEPLOYMENT)}}  # must have at least 1 deplyoment
{{$concurrentDeploymentsPerNS := MaxInt 1 (DivideFloat $desiredConcurrentDeployments $NS_COUNT)}}

# Shorten the churn phase, by doing only part of it. We churn (i.e. delete and replace) $CHURN_FRACTION of the deployments
{{$deploymentsToRecreatePerNS := MultiplyInt $concurrentDeploymentsPerNS $CHURN_FRACTION}}  # re-create this many
{{$deploymentsToKeepPerNS := SubtractInt $concurrentDeploymentsPerNS $deploymentsToRecreatePerNS}} # keep this many from round 1, running throughpout round 2
{{$expectedConcurrentPods := MultiplyInt (MultiplyInt $concurrentDeploymentsPerNS $NS_COUNT) $PODS_PER_DEPLOYMENT}}  # might be different from $desirnedConcurrentPods due to rounding etc
{{$expectedSecondsInStartupPhase := DivideInt $desiredConcurrentDeployments $targetDeploymentCreationsPerSecond}}  # expected duration of round 1
{{$expectedSecondsInChurnPhase := DivideInt (MultiplyInt $deploymentsToRecreatePerNS $NS_COUNT) $targetDeploymentCreationsPerSecond}}  # expected duration of round 2

{{$podStartTimeout := print $POD_START_TIMEOUT_MINS "m"}}


namespace:
  number: {{$NS_COUNT}}
  deleteStaleNamespaces: true # delete any old ones from previous failed CL2 runs (We seem to need this, because stuff got pick up from other test namespaces, WaitForRunningPods. Don't know why! It shouldn't work like that.)
  deleteAutomanagedNamespaces: true # delete at end of test
  enableExistingNamespaces: false # only use the automanged ones that CL2 creates for us

tuningSets:
- name: TargetCreateQps
  qpsLoad:
    qps: {{$targetDeploymentCreationsPerSecond}}
- name: TargetDeleteQps
  qpsLoad:
    qps: {{$targetDeploymentCreationsPerSecond}} # has same numerical value as create, but is separate tuning set so neither can starve the other

steps:

#### Log params ###
# Can't find a log action, but the above name should function like a log, to let us see the computeed sleep seconds
- name: Log - deployment creations {{$targetDeploymentCreationsPerSecond}}/s, , pods per deployment {{$PODS_PER_DEPLOYMENT}}, deployments to create/churn perNS {{$concurrentDeploymentsPerNS}}/{{$deploymentsToRecreatePerNS}}, number of NSs {{$NS_COUNT}}, expected seconds in startup/churn phases {{$expectedSecondsInStartupPhase}}/{{$expectedSecondsInChurnPhase}}, pod start timeout {{$podStartTimeout}}
  measurements:
  - Identifier: Dummy
    Method: Sleep
    Params:
      action: start
      duration: 1ms

### Initialize measurements
- name: Initialize measurements
  measurements:
  - Identifier: Timer
    Method: Timer
    Params:
       action: start
       label: overall duration  # can't just "declare" a timer, without starting it.

### Test, in multiple rounds of "churn creation"
- module:
    path: /churn-module.yaml
    params:
      roundNumber: 1  # this is a setup round. It has nothing to delete
      desc: Prepare initial set of deployments
      oldReplicasAfterDeletion: 0
      newReplicas: {{$concurrentDeploymentsPerNS}}
      podsPerDeployment: {{$PODS_PER_DEPLOYMENT}}
      totalPods: {{$expectedConcurrentPods}}
      timeout: {{$podStartTimeout}}
      nsCount: {{$NS_COUNT}}
- module:
    path: /churn-module.yaml
    params:
      roundNumber: 2  # this round does the real work
      desc: Do the churn test
      oldReplicasAfterDeletion: {{$deploymentsToKeepPerNS}}  # delete all the old ones EXCEPT this many
      newReplicas: {{$deploymentsToRecreatePerNS}}           # and create this many new ones
      podsPerDeployment: {{$PODS_PER_DEPLOYMENT}}
      totalPods: {{$expectedConcurrentPods}}
      timeout: {{$podStartTimeout}}
      nsCount: {{$NS_COUNT}}

{{if ne $CLEANUP 0}}      
- module:
    path: /churn-module.yaml
    params:
      roundNumber: 3
      desc: Cleanup
      oldReplicasAfterDeletion: 0
      newReplicas: 0          
      podsPerDeployment: {{$PODS_PER_DEPLOYMENT}}
      totalPods: 0
      timeout: {{$podStartTimeout}}
{{end}}

### Gather measurements
- name: Gather measurements
  measurements:
  - Identifier: Timer
    Method: Timer
    Params:
      action: gather
  
