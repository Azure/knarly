name: deployment-churn
# Churn test with deployments
# This test works like the pod-churn one, but it groups the pods together into deployments.
# Note that, for high values of PODS_PER_DEPLOYMENT its interaction with the API server will get 
# quite chunky. E.g. if it makes a deployment with 1000 pods, that's just one operation, and this test
# does not control or define the rate at which those pods get created. (I.e. this test only controls the rate
# at which _deployments_ are created.)

# input params (which be default come from override file or CL2... env vars)
{{$ACTIVE_PODS_PER_NODE := DefaultParam .CL2_PODS_PER_NODE 60}}
{{$PODS_PER_DEPLOYMENT := DefaultParam .CL2_PODS_PER_DEPLOYMENT 100}}
{{$TARGET_POD_CHURN := DefaultParam .CL2_TARGET_POD_CHURN 10}}  # i.e. target pod churn, in mutations/sec, for cluster as a whole. (create+update+delete operations per second)
{{$NS_COUNT := DefaultParam .CL2_NS_COUNT 10}}  
{{$POD_START_TIMEOUT_MINS := DefaultParam .CL2_POD_START_TIMEOUT_MINS 5}}  # how long to wait, at end of a phase, for its pods to start up
{{$CHURN_FRACTION := DefaultParam .CL2_CHURN_FRACTION 1.0}}  # Set below 1 for faster churn phase, IFF cleanup is set to 0
{{$REPEATS := DefaultParam .CL2_REPEATS 1}}  # How many times to repeat the churn phase.  Set to non-zero value for longer-running tests
{{$CLEANUP := DefaultParam .CL2_CLEANUP 1}}
# Note on CLEANUP of pods and deployments that we create.
# By default we do clean them up explicitly, because by default letting them just get deleted with the namespace is slow.
# However, if delete-collection-workers has been set to a highish value (e.g. 250) in the API server params, then 
# there is no need for us to delete them explicitly. They'l be deleted quickly when this test deletes its namespace.
# So, if you have set that parameter in your API server, then set the CL2_CLEANUP parameter to 0

# computed params
{{$desiredConcurrentPods := MultiplyInt .Nodes $ACTIVE_PODS_PER_NODE}}  #Total number of active pods for cluster
{{$targetPodCreationsPerSecond := DivideInt $TARGET_POD_CHURN 2 }}  # The divisor here is because we want half the churn to come from creates and half from deletes (there are no other pod _spec_ changes in this test, and it's only spec changes, not status changes, that count to the official definiton of churn)
{{$targetDeploymentCreationsPerSecond := DivideFloat $targetPodCreationsPerSecond $PODS_PER_DEPLOYMENT}}
{{$desiredConcurrentDeployments := MaxInt 1 (DivideFloat $desiredConcurrentPods $PODS_PER_DEPLOYMENT)}}  # must have at least 1 deplyoment
{{$concurrentDeploymentsPerNS := MaxInt 1 (DivideFloat $desiredConcurrentDeployments $NS_COUNT)}}

# Shorten the churn phase, by doing only part of it. We churn (i.e. delete and replace) $CHURN_FRACTION of the deployments
{{$deploymentsToRecreatePerNS := MultiplyInt $concurrentDeploymentsPerNS $CHURN_FRACTION}}  # re-create this many
{{$deploymentsToKeepPerNS := SubtractInt $concurrentDeploymentsPerNS $deploymentsToRecreatePerNS}} # keep this many from round 1, running throughpout round 2
{{$expectedConcurrentPods := MultiplyInt (MultiplyInt $concurrentDeploymentsPerNS $NS_COUNT) $PODS_PER_DEPLOYMENT}}  # might be different from $desirnedConcurrentPods due to rounding etc
{{$expectedSecondsInStartupPhase := DivideInt $desiredConcurrentDeployments $targetDeploymentCreationsPerSecond}}  # expected duration of round 1
{{$expectedSecondsInChurnPhase := DivideInt (MultiplyInt $deploymentsToRecreatePerNS $NS_COUNT) $targetDeploymentCreationsPerSecond}}  # expected duration of round 2

{{$podStartTimeout := print $POD_START_TIMEOUT_MINS "m"}}

{{if and (ne $CLEANUP 0) (ne $CHURN_FRACTION 1.0)}}
   # there is nothing actually called error:.  The next line will force a compliation error. Please read what it says, if you get the error
   error: when using explicit cleanup, churn fraction must be 1.0 # otherwise the cleanup doesn't work
{{end}}

{{if and (ne $REPEATS 1) (ne $CHURN_FRACTION 1.0)}}
   # there is nothing actually called error:.  The next line will force a compliation error. Please read what it says, if you get the error
   error: when using more than 1 repeat, churn fraction must be 1.0 # otherwise our churn code doesn't work properly, and there's no point anywany, because the whole point of $REPEATS is longer test, and the whole point of Churn Fraction < 1 is shorter tests
{{end}}

namespace:
  number: {{$NS_COUNT}}
  deleteStaleNamespaces: true # delete any old ones from previous failed CL2 runs (We seem to need this, because stuff got pick up from other test namespaces, WaitForRunningPods. Don't know why! It shouldn't work like that.)
  deleteAutomanagedNamespaces: true # delete at end of test
  enableExistingNamespaces: false # only use the automanged ones that CL2 creates for us

tuningSets:
- name: TargetCreateQps
  qpsLoad:
    qps: {{$targetDeploymentCreationsPerSecond}}
- name: TargetDeleteQps
  qpsLoad:
    qps: {{$targetDeploymentCreationsPerSecond}} # has same numerical value as create, but is separate tuning set so neither can starve the other

steps:

#### Log params ###
# Can't find a log action, but the above name should function like a log, to let us see the computeed sleep seconds
- name: Log - deployment creations {{$targetDeploymentCreationsPerSecond}}/s, pods per deployment {{$PODS_PER_DEPLOYMENT}}, deployments to create/churn per namespace {{$concurrentDeploymentsPerNS}}/{{$deploymentsToRecreatePerNS}}, number of namespaces {{$NS_COUNT}}, expected seconds in startup/one churn phase {{$expectedSecondsInStartupPhase}}/{{$expectedSecondsInChurnPhase}}, num churn phases {{$REPEATS}} pod start timeout {{$podStartTimeout}}
  measurements:
  - Identifier: Dummy
    Method: Sleep
    Params:
      action: start
      duration: 1ms

### Initialize measurements
- name: Initialize measurements
  measurements:
  - Identifier: Timer
    Method: Timer
    Params:
       action: start
       label: overall duration  # can't just "declare" a timer, without starting it.

### Create initial set of pods
- module:
    path: /churn-module.yaml
    params:
      roundNumber: 1  # this is a setup round. It has nothing to delete
      desc: Prepare initial set of deployments
      oldReplicasAfterDeletion: 0
      newReplicas: {{$concurrentDeploymentsPerNS}}
      podsPerDeployment: {{$PODS_PER_DEPLOYMENT}}
      totalPods: {{$expectedConcurrentPods}}
      timeout: {{$podStartTimeout}}
      nsCount: {{$NS_COUNT}}

### One or more rounds of churn
{{range $i := Loop $REPEATS}}
- module:
    path: /churn-module.yaml
    params:
      roundNumber: {{AddInt $i 2}}  # add two to the range variable because our first churn round is numbered 2
      desc: Do churn
      oldReplicasAfterDeletion: {{$deploymentsToKeepPerNS}}  # delete all the old ones EXCEPT this many
      newReplicas: {{$deploymentsToRecreatePerNS}}           # and create this many new ones
      podsPerDeployment: {{$PODS_PER_DEPLOYMENT}}
      totalPods: {{$expectedConcurrentPods}}
      timeout: {{$podStartTimeout}}
      nsCount: {{$NS_COUNT}}
{{end}}      

### cleanup
{{if ne $CLEANUP 0}}      
- module:
    path: /churn-module.yaml
    params:
      roundNumber: {{AddInt $REPEATS 2}}  # add two, because we use 1-based round numbers
      desc: Cleanup
      oldReplicasAfterDeletion: 0
      newReplicas: 0          
      podsPerDeployment: {{$PODS_PER_DEPLOYMENT}}
      totalPods: 0
      timeout: {{$podStartTimeout}}
      nsCount: {{$NS_COUNT}}
{{end}}

### Gather measurements
- name: Gather measurements
  measurements:
  - Identifier: Timer
    Method: Timer
    Params:
      action: gather
  
